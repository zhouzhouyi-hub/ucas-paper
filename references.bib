@article{fewshot,
  title={A theoretical analysis of the number of shots in few-shot learning},
  author={Cao, Tianshi and Law, Marc and Fidler, Sanja},
  journal={arXiv preprint arXiv:1909.11722},
  year={2019}
}

@INPROCEEDINGS{mfcc,
  author={Vimal, B. and Surya, Muthyam and Darshan and Sridhar, V.S. and Ashok, Asha},
  booktitle={International Conference on Computing Communication and Networking Technologies (ICCCNT)}, 
  title={{MFCC} Based Audio Classification Using Machine Learning}, 
  year={2021},
  volume={},
  number={},
  pages={1-4},
  keywords={Support vector machines;Training;Emotion recognition;Machine learning algorithms;Speech recognition;Feature extraction;Classification algorithms;RAVDESS dataset;Emotion recognition;Decission Tree;Support Vector Machine (SVM);Random Forest},
  doi={10.1109/ICCCNT51525.2021.9579881}}

@misc{elizalde_clap_2022,
	title = {{CLAP}: {Learning} {Audio} {Concepts} {From} {Natural} {Language} {Supervision}},
	shorttitle = {{CLAP}},
	url = {http://arxiv.org/abs/2206.04769},
	abstract = {Mainstream Audio Analytics models are trained to learn under the paradigm of one class label to many recordings focusing on one task. Learning under such restricted supervision limits the ﬂexibility of models because they require labeled audio for training and can only predict the predeﬁned categories. Instead, we propose to learn audio concepts from natural language supervision. We call our approach Contrastive Language-Audio Pretraining (CLAP), which learns to connect language and audio by using two encoders and a contrastive learning to bring audio and text descriptions into a joint multimodal space. We trained CLAP with 128k audio and text pairs and evaluated it on 16 downstream tasks across 8 domains, such as Sound Event Classiﬁcation, Music tasks, and Speech-related tasks. Although CLAP was trained with signiﬁcantly less pairs than similar computer vision models, it establishes SoTA for Zero-Shot performance. Additionally, we evaluated CLAP in a supervised learning setup and achieve SoTA in 5 tasks. Hence, CLAP’s Zero-Shot capability removes the need of training with class labels, enables ﬂexible class prediction at inference time, and generalizes to multiple downstream tasks.},
	language = {en},
	urldate = {2023-09-23},
	publisher = {arXiv},
	author = {Elizalde, Benjamin and Deshmukh, Soham and Ismail, Mahmoud Al and Wang, Huaming},
	month = jun,
	year = {2022},
	note = {arXiv:2206.04769 [cs, eess]},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {Elizalde et al. - 2022 - CLAP Learning Audio Concepts From Natural Languag.pdf:C\:\\Users\\YUSHAH\\Zotero\\storage\\BSCJIV5H\\Elizalde et al. - 2022 - CLAP Learning Audio Concepts From Natural Languag.pdf:application/pdf},
}



@ARTICLE{soundevent,
  author={McLoughlin, Ian and Zhang, Haomin and Xie, Zhipeng and Song, Yan and Xiao, Wei},
  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing}, 
  title={Robust Sound Event Classification Using Deep Neural Networks}, 
  year={2015},
  volume={23},
  number={3},
  pages={540-552},
  keywords={Vectors;Feature extraction;Support vector machines;Spectrogram;Speech;Speech processing;Auditory system;Auditory event detection;machine hearing},
  doi={10.1109/TASLP.2015.2389618}}


@article{esc1,
author = {Tripathi, Achyut Mani and Mishra, Aakansha},
title = {Environment sound classification using an attention-based residual neural network},
year = {2021},
issue_date = {Oct 2021},
publisher = {Elsevier Science Publishers B. V.},
address = {NLD},
volume = {460},
number = {C},
issn = {0925-2312},
url = {https://doi.org/10.1016/j.neucom.2021.06.031},
doi = {10.1016/j.neucom.2021.06.031},
journal = {Neurocomput.},
month = {oct},
pages = {409–423},
numpages = {15},
keywords = {Residual network, Environmental sound classification, Explainable, Convolutional neural network, Attention mechanism}
}


@article{niizumi_byol_2023,
        title = {{BYOL} for {Audio}: {Exploring} {Pre}-{Trained} {General}-{Purpose} {Audio} {Representations}},
        volume = {31},
        issn = {2329-9290, 2329-9304},
        shorttitle = {{BYOL} for {Audio}},
        url = {https://ieeexplore.ieee.org/document/9944865/},
        doi = {10.1109/TASLP.2022.3221007},
        language = {en},
        urldate = {2023-09-19},
        journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
        author = {Niizumi, Daisuke and Takeuchi, Daiki and Ohishi, Yasunori and Harada, Noboru and Kashino, Kunio},
        year = {2023},
        pages = {137--151},
        file = {Niizumi et al. - 2023 - BYOL for Audio Exploring Pre-Trained General-Purp.pdf:C\:\\Users\\YUSHAH\\Zotero\\storage\\3RTEFSDI\\Niizumi et al. - 2023 - BYOL for Audio
Exploring Pre-Trained General-Purp.pdf:application/pdf},
}



@misc{turian2022hear,
      title={{HEAR: Holistic Evaluation of Audio Representations}}, 
      author={Joseph Turian and Jordie Shier and Humair Raj Khan and Bhiksha Raj and Björn W. Schuller and Christian J. Steinmetz and Colin Malloy and George Tzanetakis and Gissel Velarde and Kirk McNally and Max Henry and Nicolas Pinto and Camille Noufi and Christian Clough and Dorien Herremans and Eduardo Fonseca and Jesse Engel and Justin Salamon and Philippe Esling and Pranay Manocha and Shinji Watanabe and Zeyu Jin and Yonatan Bisk},
      year={2022},
      eprint={2203.03022},
      archivePrefix={arXiv},
      primaryClass={cs.SD}
}


@article{fastmoving0,
  title={The immediate impact of different types of television on young children's executive function},
  author={Lillard, Angeline S and Peterson, Jennifer},
  journal={Pediatrics},
  volume={128},
  number={4},
  pages={644--649},
  year={2011},
  publisher={American Academy of Pediatrics Elk Grove Village, IL, USA}
}

@misc{ytusagedaily, title={{YouTube} stats: Everything you need to know in 2024!}, url={https://www.wyzowl.com/youtube-stats/}, journal={Wyzowl}, publisher={Wyzowl}, author={Hayes, Adam}, year={2024}, month={Jan}, urldate = {2024-01-27}} 


@misc{lawsuit, title={States sue {Meta} claiming its social platforms are addictive and harm children’s mental health}, url={https://apnews.com/article/instagram-facebook-children-teens-harms-lawsuit-attorney-general-1805492a38f7cee111cbb865cc786c28}, journal={AP News}, publisher={AP News}, author={Ortutay, Barbara}, year={2023}, month={Oct},   urldate = {2024-01-27}} 

@article{oswald2020psychological,
  title={Psychological impacts of “screen time” and “green time” for children and adolescents: A systematic scoping review},
  author={Oswald, Tassia K and Rumbold, Alice R and Kedzior, Sophie GE and Moore, Vivienne M},
  journal={PloS one},
  volume={15},
  number={9},
  pages={e0237725},
  year={2020},
  publisher={Public Library of Science San Francisco, CA USA}
}

@misc{TikTokContentMod, title={Our approach to content moderation}, url={https://www.tiktok.com/transparency/en-us/content-moderation/}, author={TikTok}, journal={TikTok}, year={2024}, urldate = {2024-01-27}} 

@misc{YouTubeContentMod, title={How does {YouTube} manage harmful content?}, url={https://www.youtube.com/howyoutubeworks/our-commitments/managing-harmful-content/}, journal={YouTube}, author={YouTube}, publisher={YouTube}, year={2024}, urldate = {2024-01-27}} 

@article{babysitting_tool,
  title={Mothers’ and father’s perceptions of the risks and benefits of screen time and physical activity during early childhood: a qualitative study},
  author={Hinkley, Trina and McCann, Jennifer R},
  journal={BMC Public Health},
  volume={18},
  number={1},
  pages={1--8},
  year={2018},
  publisher={BioMed Central}
}

@misc{Shewale_2024a, title={Video marketing statistics in 2024 (usage, Roi and More)}, url={https://www.demandsage.com/video-marketing-statistics}, journal={demandsage}, author={Shewale, Rohit}, year={2024}, month={Jan}, urldate = {2024-01-27}} 


@inproceedings{urbansound,
author = {Salamon, Justin and Jacoby, Christopher and Bello, Juan Pablo},
title = {A Dataset and Taxonomy for Urban Sound Research},
year = {2014},
isbn = {9781450330633},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2647868.2655045},
doi = {10.1145/2647868.2655045},
booktitle = {Proceedings of the 22nd ACM International Conference on Multimedia},
pages = {1041–1044},
numpages = {4},
keywords = {urban sound, taxonomy, dataset, classification},
location = {Orlando, Florida, USA},
series = {MM '14}
}

@inproceedings{esc2,
  title = {{ESC}: {Dataset} for {Environmental Sound Classification}},
  author = {Piczak, Karol J.},
  booktitle = {Proceedings of the 23rd {Annual ACM Conference} on {Multimedia}},
  date = {2015-10-13},
  year = {2015},
  url = {http://dl.acm.org/citation.cfm?doid=2733373.2806390},
  doi = {10.1145/2733373.2806390},
  location = {{Brisbane, Australia}},
  isbn = {978-1-4503-3459-4},
  publisher = {{ACM Press}},
  pages = {1015--1018}
}

@misc{preferred_genre, title={Preferred digital video content by genre in the U.S. 2023}, url={https://www.statista.com/forecasts/997166/preferred-digital-video-content-by-genre-in-the-us}, journal={Statista}, author={Bashir, Umair}, year={2023}, month={Nov}, urldate = {2024-01-25}} 

@inproceedings{samba, title={Samba: Identifying Inappropriate Videos for Young Children on {YouTube}}, author={Le, Binh M and Tandon, Rajat and Oinar, Chingis}, year={2022}, booktitle = {Proceedings of the ACM International Conference on Information and Knowledge Management}, language={en} }


 @article{fast_motion_psycho, title={The Immediate Impact of Different Types of Television on Young Children’s Executive Function}, volume={128}, ISSN={0031-4005, 1098-4275}, DOI={10.1542/peds.2010-1919}, number={4}, journal={Pediatrics}, author={Lillard, Angeline S. and Peterson, Jennifer}, year={2011}, month={10}, pages={644–649}, language={en} }

 @article{loud_noise_psycho, title={Does noise affect learning? {A} short review on noise effects on cognitive performance in children}, volume={4}, ISSN={1664-1078}, url={http://journal.frontiersin.org/article/10.3389/fpsyg.2013.00578/abstract}, DOI={10.3389/fpsyg.2013.00578}, journal={Frontiers in Psychology}, author={Klatte, Maria and Bergström, Kirstin and Lachmann, Thomas}, year={2013}, language={en} }

 @article{violent_cartoons_psycho, title={Children’s reactions to cartoon violence}, volume={8}, ISSN={0047-228X}, DOI={10.1080/15374417909532878}, number={1}, journal={Journal of Clinical Child Psychology}, author={Hapkiewicz, Walter G.}, year={1979}, month={03}, pages={30–34}, language={en} }


@INPROCEEDINGS{litrevpaper,
  author={Aggarwal, Sajal and Vishwakarma, Dinesh Kumar},
  booktitle={2023 4th IEEE Global Conference for Advancement in Technology (GCAT)}, 
  title={Protecting our Children from the Dark Corners of YouTube: A Cutting-Edge Analysis}, 
  year={2023},
  volume={},
  number={},
  pages={1-5},
  keywords={Surveys;Visualization;Video on demand;Social networking (online);Filtering;Films;Media;Unsafe content detection;content forensics;YouTube;child safety;video classification;social media analysis;deep learning},
  doi={10.1109/GCAT59970.2023.10353306}}


@article{khalil2021,
title = {Detection of Violence in Cartoon Videos Using Visual Features},
journal = {Procedia Computer Science},
volume = {192},
pages = {4962-4971},
year = {2021},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 25th International Conference KES2021},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.09.274},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921020135},
author = {Tahira Khalil and Javed Iqbal Bangash and Abdul Waheed Khan and Saima Anwar Lashari and Abdullah Khan and Dzati Athiar Ramli}
}


@misc{vanilla_clip,
      title={Learning Transferable Visual Models From Natural Language Supervision}, 
      author={Alec Radford and Jong Wook Kim and Chris Hallacy and Aditya Ramesh and Gabriel Goh and Sandhini Agarwal and Girish Sastry and Amanda Askell and Pamela Mishkin and Jack Clark and Gretchen Krueger and Ilya Sutskever},
      year={2021},
      eprint={2103.00020},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{audio_clip,
      title={AudioCLIP: Extending {CLIP} to Image, Text and Audio}, 
      author={Andrey Guzhov and Federico Raue and Jörn Hees and Andreas Dengel},
      year={2021},
      eprint={2106.13043},
      archivePrefix={arXiv},
      primaryClass={cs.SD}
}


@misc{resnet50,
      title={Deep Residual Learning for Image Recognition}, 
      author={Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
      year={2015},
      eprint={1512.03385},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@article{ahmed2023icmla,
  title={The Potential of Vision-Language Models for Content Moderation of Children's Videos},
  author={Ahmed, Syed Hammad and Hu, Shengnan and Sukthankar, Gita},
  journal={arXiv preprint arXiv:2312.03936},
  year={2023}
}

@article{ahmed2023flairs, title={Malicious or Benign? {Towards} Effective Content Moderation for Children’s Videos}, volume={36}, url={https://journals.flvc.org/FLAIRS/article/view/133315}, DOI={10.32473/flairs.36.133315},  number={1}, journal={The International FLAIRS Conference Proceedings}, author={Ahmed, Syed Hammad and Khan, Muhammad Junaid and Qaisar, Hafiz Muhammad Umer and Sukthankar, Gita}, year={2023}, month={May} }

@inproceedings{esresnext,
  title={{Esresne (x) t-fbsp}: Learning robust time-frequency transformation of audio},
  author={Guzhov, Andrey and Raue, Federico and Hees, J{\"o}rn and Dengel, Andreas},
  booktitle={International Joint Conference on Neural Networks (IJCNN)},
  pages={1--8},
  year={2021},
  organization={IEEE}
}

@misc{esresnet,
      title={ESResNet: Environmental Sound Classification Based on Visual Domain Models}, 
      author={Andrey Guzhov and Federico Raue and Jörn Hees and Andreas Dengel},
      year={2020},
      eprint={2004.07301},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@INPROCEEDINGS{image_net,
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Kai Li and Li Fei-Fei},
  booktitle={IEEE Conference on Computer Vision and Pattern Recognition}, 
  title={{ImageNet}: A large-scale hierarchical image database}, 
  year={2009},
  volume={},
  number={},
  pages={248-255},
  keywords={Large-scale systems;Image databases;Explosions;Internet;Robustness;Information retrieval;Image retrieval;Multimedia databases;Ontologies;Spine},
  doi={10.1109/CVPR.2009.5206848}}

@inproceedings{audio_set,
  title={Audio set: An ontology and human-labeled dataset for audio events},
  author={Gemmeke, Jort F and Ellis, Daniel PW and Freedman, Dylan and Jansen, Aren and Lawrence, Wade and Moore, R Channing and Plakal, Manoj and Ritter, Marvin},
  booktitle={IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={776--780},
  year={2017},
  organization={IEEE}
}

@inproceedings{tahir_audio,
author = {Tahir, Rashid and Ahmed, Faizan and Saeed, Hammas and Ali, Shiza and Zaffar, Fareed and Wilson, Christo},
title = {Bringing the kid back into {YouTube Kids}: detecting inappropriate content on video streaming platforms},
year = {2020},
isbn = {9781450368681},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3341161.3342913},
doi = {10.1145/3341161.3342913},
pages = {464–469},
numpages = {6},
location = {Vancouver, British Columbia, Canada},
series = {ASONAM '19}
}

@InProceedings{algh_audio,
author="Alghowinem, Sharifa",
editor="Arai, Kohei
and Kapoor, Supriya
and Bhatia, Rahul",
title="A Safer {YouTube Kids}: An Extra Layer of Content Filtering Using Automated Multimodal Analysis",
booktitle="Intelligent Systems and Applications",
year="2019",
publisher="Springer International Publishing",
address="Cham",
pages="294--308",
isbn="978-3-030-01054-6"
}

@ARTICLE{user_comm1,
  author={Yousaf, Kanwal and Nawaz, Tabassam},
  journal={IEEE Access}, 
  title={A Deep Learning-Based Approach for Inappropriate Content Detection and Classification of YouTube Videos}, 
  year={2022},
  volume={10},
  number={},
  pages={16283-16298},
  keywords={Videos;Feature extraction;Deep learning;Support vector machines;Convolutional neural networks;Visualization;Classification algorithms;Deep learning;social media analysis;video classification;bidirectional LSTM;CNN;EfficientNet},
  doi={10.1109/ACCESS.2022.3147519}
}

@inproceedings{user_comm2,
author = {Singh Albert, Sr and Kaushal, Rishabh and Buduru, Arun Balaji and Kumaraguru, Ponnurangam},
year = {2019},
month = {04},
pages = {2104-2111},
title = {KidsGUARD: fine grained approach for child unsafe video representation and detection},
isbn = {978-1-4503-5933-7},
journal = {SAC '19: Proceedings of the 34th ACM/SIGAPP Symposium on Applied Computing},
doi = {10.1145/3297280.3297487}
}

@inproceedings{vid_ads,
author = {Liu, Jeffrey and Tandon, Rajat and Durairaj, Uma and Guo, Jiani and Zahabizadeh, Spencer and Ilango, Sanjana and Tang, Jeremy and Gupta, Neelesh and Zhou, Zoe and Mirkovic, Jelena},
year = {2022},
month = {08},
pages = {},
title = {Did your child get disturbed by an inappropriate advertisement on YouTube?}
}

@inproceedings{vid1,
author = {Ishikawa, Akari and Bollis, Edson and Avila, Sandra},
year = {2019},
month = {05},
pages = {},
title = {Combating the Elsagate Phenomenon: Deep Learning Architectures for Disturbing Cartoons},
doi = {10.1109/IWBF.2019.8739202}
}

@misc{textonlyprompting,
      title={Prompting Visual-Language Models for Efficient Video Understanding}, 
      author={Chen Ju and Tengda Han and Kunhao Zheng and Ya Zhang and Weidi Xie},
      year={2022},
      eprint={2112.04478},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@inproceedings{nlp-prompt1,
    title = "{A}uto{P}rompt: {E}liciting {K}nowledge from {L}anguage {M}odels with {A}utomatically {G}enerated {P}rompts",
    author = "Shin, Taylor  and
      Razeghi, Yasaman  and
      Logan IV, Robert L.  and
      Wallace, Eric  and
      Singh, Sameer",
    editor = "Webber, Bonnie  and
      Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
    booktitle = "Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.346",
    doi = "10.18653/v1/2020.emnlp-main.346",
    pages = "4222--4235"
}

@misc{nlp-prompt2,
      title={How Can We Know What Language Models Know?}, 
      author={Zhengbao Jiang and Frank F. Xu and Jun Araki and Graham Neubig},
      year={2020},
      eprint={1911.12543},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{nlp-prompt3,
      title={Factual Probing Is [MASK]: Learning vs. Learning to Recall}, 
      author={Zexuan Zhong and Dan Friedman and Danqi Chen},
      year={2021},
      eprint={2104.05240},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{nlp-prompt4,
      title={Supervision Exists Everywhere: A Data Efficient Contrastive Language-Image Pre-training Paradigm}, 
      author={Yangguang Li and Feng Liang and Lichen Zhao and Yufeng Cui and Wanli Ouyang and Jing Shao and Fengwei Yu and Junjie Yan},
      year={2022},
      eprint={2110.05208},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@article{visual_prompt_1st,
  title={Visual prompting: Modifying pixel space to adapt pre-trained models},
  author={Bahng, Hyojin and Jahanian, Ali and Sankaranarayanan, Swami and Isola, Phillip},
  journal={arXiv preprint arXiv:2203.17274},
  volume={3},
  pages={11--12},
  year={2022}
}

@inproceedings{1,
author = {Gkolemi, Myrsini and Papadopoulos, Panagiotis and Markatos, Evangelos and Kourtellis, Nicolas},
title = {YouTubers Not MadeForKids: Detecting Channels Sharing Inappropriate Videos Targeting Children},
year = {2022},
isbn = {9781450391917},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3501247.3531556},
doi = {10.1145/3501247.3531556},
booktitle = {14th ACM Web Science Conference 2022},
pages = {370–381},
numpages = {12},
location = {Barcelona, Spain},
series = {WebSci '22}
}

 @inproceedings{alshamrani, address={Ljubljana Slovenia}, title={Hate, Obscenity, and Insults: Measuring the Exposure of Children to Inappropriate Comments in YouTube}, ISBN={978-1-4503-8313-4}, url={https://dl.acm.org/doi/10.1145/3442442.3452314}, DOI={10.1145/3442442.3452314}, booktitle={Companion Proceedings of the Web Conference 2021}, publisher={ACM}, author={Alshamrani, Sultan and Abusnaina, Ahmed and Abuhamad, Mohammed and Nyang, Daehun and Mohaisen, David}, year={2021}, month={Apr}, pages={508–515}, language={en} }


 @article{yousafnawazuettaxila, title={A Deep Learning-Based Approach for Inappropriate Content Detection and Classification of YouTube Videos}, volume={10}, ISSN={2169-3536}, DOI={10.1109/ACCESS.2022.3147519},  journal={IEEE Access}, author={Yousaf, Kanwal and Nawaz, Tabassam}, year={2022}, pages={16283–16298}, language={en} }

 @article{Papadamou, title={Disturbed {YouTube for Kids}: Characterizing and Detecting Inappropriate Videos Targeting Young Children}, url={http://arxiv.org/abs/1901.07046}, note={arXiv:1901.07046 [cs]}, number={arXiv:1901.07046}, publisher={arXiv}, author={Papadamou, Kostantinos and Papasavva, Antonis and Zannettou, Savvas and Blackburn, Jeremy and Kourtellis, Nicolas and Leontiadis, Ilias and Stringhini, Gianluca and Sirivianos, Michael}, year={2021}, month={Sep}, language={en} }

@misc{FTC_2022, title={YouTube channel owners: Is your content directed to children?}, url={https://www.ftc.gov/business-guidance/blog/2019/11/youtube-channel-owners-your-content-directed-children}, journal={Federal Trade Commission}, publisher={Federal Trade Commission}, author={Technology, The Office of and Cohen, Kristin and lu, Sarah and Staff, FTC and hossain, Amzad and malihi, Reut and Haley, Clyde L and Wilfrido, German and matak, Bum jaise atak and Heather and et al.}, year={2022}, month={Jun}} 

@inproceedings{ju2022prompting,
  title={Prompting visual-language models for efficient video understanding},
  author={Ju, Chen and Han, Tengda and Zheng, Kunhao and Zhang, Ya and Xie, Weidi},
  booktitle={European Conference on Computer Vision},
  pages={105--124},
  year={2022},
  organization={Springer}
}

@article{chuttur, title={A multi-modal approach to detect inappropriate cartoon video contents using deep learning networks}, volume={81}, ISSN={1380-7501, 1573-7721}, DOI={10.1007/s11042-022-12709-2}, number={12}, journal={Multimedia Tools and Applications}, author={Chuttur, M. Y. and Nazurally, A.}, year={2022}, month={May}, pages={16881–16900}, language={en} }


@article{kidstube, title={KidsTube: Detection, Characterization and Analysis of Child Unsafe Content and Promoters on YouTube}, url={http://arxiv.org/abs/1608.05966}, note={arXiv:1608.05966 [cs]}, number={arXiv:1608.05966}, publisher={arXiv}, author={Kaushal, Rishabh and Saha, Srishty and Bajaj, Payal and Kumaraguru, Ponnurangam}, year={2016}, month={Aug}, language={en} }


@online{EUdirective2018,
  author = {{European Union}},
  title = {{Directive 2018/1808/EU}},
  year = {2018},
  url = {https://eur-lex.europa.eu/legal-content/EN/ALL/?uri=CELEX:32018L1808},
}

@misc{COPPA1998,
  title = {Children's Online Privacy Protection Act of 1998},
  author ={COPPA},
  year = {1998},
  howpublished = {Public Law 105-277, 112 Stat. 2681-728},
}

 @article{coop, title={CoOP: Learning to Prompt for Vision-Language Models}, volume={130}, ISSN={0920-5691, 1573-1405}, DOI={10.1007/s11263-022-01653-1}, number={9}, journal={International Journal of Computer Vision}, author={Zhou, Kaiyang and Yang, Jingkang and Loy, Chen Change and Liu, Ziwei}, year={2022}, month=sep, pages={2337–2348}, language={en} }

 @article{cocoop, title={CoCoOp: Conditional Prompt Learning for Vision-Language Models}, url={http://arxiv.org/abs/2203.05557}, number={arXiv:2203.05557}, publisher={arXiv}, author={Zhou, Kaiyang and Yang, Jingkang and Loy, Chen Change and Liu, Ziwei}, year={2022}, month=oct, language={en} }


@misc{jia2022visual_16_deep,
      title={Visual Prompt Tuning}, 
      author={Menglin Jia and Luming Tang and Bor-Chun Chen and Claire Cardie and Serge Belongie and Bharath Hariharan and Ser-Nam Lim},
      year={2022},
      eprint={2203.12119},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{wang2022dualprompt_38,
      title={DualPrompt: Complementary Prompting for Rehearsal-free Continual Learning}, 
      author={Zifeng Wang and Zizhao Zhang and Sayna Ebrahimi and Ruoxi Sun and Han Zhang and Chen-Yu Lee and Xiaoqi Ren and Guolong Su and Vincent Perot and Jennifer Dy and Tomas Pfister},
      year={2022},
      eprint={2204.04799},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{zhang2022neural_47,
      title={Neural Prompt Search}, 
      author={Yuanhan Zhang and Kaiyang Zhou and Ziwei Liu},
      year={2022},
      eprint={2206.04673},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{zhu2024promptaligned_51,
      title={Prompt-aligned Gradient for Prompt Tuning}, 
      author={Beier Zhu and Yulei Niu and Yucheng Han and Yue Wu and Hanwang Zhang},
      year={2024},
      eprint={2205.14865},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

 @article{maple, title={{MaPLe}: Multi-modal Prompt Learning}, url={http://arxiv.org/abs/2210.03117}, abstractNote={Pre-trained vision-language (V-L) models such as CLIP have shown excellent generalization ability to downstream tasks. However, they are sensitive to the choice of input text prompts and require careful selection of prompt templates to perform well. Inspired by the Natural Language Processing (NLP) literature, recent CLIP adaptation approaches learn prompts as the textual inputs to ﬁne-tune CLIP for downstream tasks. We note that using prompting to adapt representations in a single branch of CLIP (language or vision) is sub-optimal since it does not allow the ﬂexibility to dynamically adjust both representation spaces on a downstream task. In this work, we propose Multi-modal Prompt Learning (MaPLe) for both vision and language branches to improve alignment between the vision and language representations. Our design promotes strong coupling between the vision-language prompts to ensure mutual synergy and discourages learning independent uni-modal solutions. Further, we learn separate prompts across different early stages to progressively model the stage-wise feature relationships to allow rich context learning. We evaluate the effectiveness of our approach on three representative tasks of generalization to novel classes, new target datasets and unseen domain shifts. Compared with the state-of-the-art method CoCoOp, MaPLe exhibits favorable performance and achieves an absolute gain of 3.45% on novel classes and 2.72% on overall harmonic-mean, averaged over 11 diverse image recognition datasets. Our code and pre-trained models are available at https://github.com/muzairkhattak/multimodalprompt-learning.}, note={arXiv:2210.03117 [cs]}, number={arXiv:2210.03117}, publisher={arXiv}, author={Khattak, Muhammad Uzair and Rasheed, Hanoona and Maaz, Muhammad and Khan, Salman and Khan, Fahad Shahbaz}, year={2023}, month=apr, language={en} }

 @article{vifi, title={{ViFi-CLIP}: Fine-tuned {CLIP} Models are Efficient Video Learners}, url={http://arxiv.org/abs/2212.03640}, abstractNote={Large-scale multi-modal training with image-text pairs imparts strong generalization to CLIP model. Since training on a similar scale for videos is infeasible, recent approaches focus on the effective transfer of image-based CLIP to the video domain. In this pursuit, new parametric modules are added to learn temporal information and inter-frame relationships which require meticulous design efforts. Furthermore, when the resulting models are learned on videos, they tend to overfit on the given task distribution and lack in generalization aspect. This begs the following question: How to effectively transfer image-level CLIP representations to videos? In this work, we show that a simple Video Fine-tuned CLIP (ViFi-CLIP) baseline is generally sufficient to bridge the domain gap from images to videos. Our qualitative analysis illustrates that the frame-level processing from CLIP image-encoder followed by feature pooling and similarity matching with corresponding text embeddings helps in implicitly modeling the temporal cues within ViFi-CLIP. Such fine-tuning helps the model to focus on scene dynamics, moving objects and inter-object relationships. For low-data regimes where full fine-tuning is not viable, we propose a `bridge and prompt’ approach that first uses fine-tuning to bridge the domain gap and then learns prompts on language and vision side to adapt CLIP representations. We extensively evaluate this simple yet strong baseline on zero-shot, base-to-novel generalization, few-shot and fully supervised settings across five video benchmarks. Our code is available at https://github.com/muzairkhattak/ViFi-CLIP.}, note={arXiv:2212.03640 [cs]}, number={arXiv:2212.03640}, publisher={arXiv}, author={Rasheed, Hanoona and Khattak, Muhammad Uzair and Maaz, Muhammad and Khan, Salman and Khan, Fahad Shahbaz}, year={2023}, month=mar, language={en} }

 @article{vita, title={{Vita-CLIP}: Video and text adaptive {CLIP} via Multimodal Prompting}, url={http://arxiv.org/abs/2304.03307}, abstractNote={Adopting contrastive image-text pretrained models like CLIP towards video classiﬁcation has gained attention due to its cost-effectiveness and competitive performance. However, recent works in this area face a trade-off. Finetuning the pretrained model to achieve strong supervised performance results in low zero-shot generalization. Similarly, freezing the backbone to retain zero-shot capability causes signiﬁcant drop in supervised accuracy. Because of this, recent works in literature typically train separate models for supervised and zero-shot action recognition. In this work, we propose a multimodal prompt learning scheme that works to balance the supervised and zero-shot performance under a single uniﬁed training. Our prompting approach on the vision side caters for three aspects: 1) Global video-level prompts to model the data distribution; 2) Local frame-level prompts to provide per-frame discriminative conditioning; and 3) a summary prompt to extract a condensed video representation. Additionally, we deﬁne a prompting scheme on the text side to augment the textual context. Through this prompting scheme, we can achieve state-of-the-art zeroshot performance on Kinetics-600, HMDB51 and UCF101 while remaining competitive in the supervised setting. By keeping the pretrained backbone frozen, we optimize a much lower number of parameters and retain the existing general representation which helps achieve the strong zero-shot performance. Our codes/models are released at https://github.com/TalalWasim/Vita-CLIP..}, note={arXiv:2304.03307 [cs, eess]}, number={arXiv:2304.03307}, publisher={arXiv}, author={Wasim, Syed Talal and Naseer, Muzammal and Khan, Salman and Khan, Fahad Shahbaz and Shah, Mubarak}, year={2023}, month=apr, language={en} }


@misc{FTC_2024, title={The Federal Register}, url={https://www.federalregister.gov/documents/2024/01/11/2023-28569/childrens-online-privacy-protection-rule}, journal={Federal Register}, author={FTC}, year={2024}, month={Jan}}

@misc{Corbet, title={Dropping the timer tick — for real this time}, url={https://lwn.net/Articles/659490/https://lwn.net/Articles/659490/},  author={J. Corbet}} 